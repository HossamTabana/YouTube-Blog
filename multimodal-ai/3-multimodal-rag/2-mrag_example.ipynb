{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec9fff1-5b49-4e93-8e28-0e70f4b98060",
   "metadata": {},
   "source": [
    "# Multimodal Article Question Answering Assistant\n",
    "\n",
    "Code authored by: Shaw Talebi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6416eb-18cb-457c-824d-fe75a80d100e",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10ee99e2-92f4-46db-a89d-ea962d0a54a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from functions import *\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch import load, matmul, argsort\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35610181-eb5a-45ea-bc98-f7077651c192",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80cc0142-e1fc-4235-b511-07752d300180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article contents\n",
    "text_content_list = load_from_json('data/text_content.json')\n",
    "image_content_list = load_from_json('data/image_content.json')\n",
    "\n",
    "# load embeddings\n",
    "text_embeddings = load('data/text_embeddings.pt', weights_only=True)\n",
    "image_embeddings = load('data/image_embeddings.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15b1b808-4118-4ee7-bdc2-35e7d34f5f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([86, 512])\n",
      "torch.Size([17, 512])\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings.shape)\n",
    "print(image_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd124a70-bdcd-4c70-a26e-f96308b0a7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article_title': 'Multimodal Models\\u200a—\\u200aLLMs that can see and hear',\n",
       " 'section': 'Path 1: LLM +\\xa0Tools',\n",
       " 'text': 'The key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training.'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_content_list[49]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c20bc30-7448-4c2b-8cd8-3916aa0f4077",
   "metadata": {},
   "source": [
    "### query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "291b7472-7a41-4040-a1bf-618072e326b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query\n",
    "query = \"What is CLIP's contrastive loss function?\"\n",
    "# query = \"What are the three paths described for making LLMs multimodal?\"\n",
    "# query = \"What is an intuitive explanation of multimodal embeddings?\"\n",
    "\n",
    "# embed query\n",
    "query_embed = embed_text(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f721f8d3-47eb-4587-8246-5651272a4678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4920cb4b-e0be-49b4-a4e8-e8ff87e84b4e",
   "metadata": {},
   "source": [
    "### Multimodal search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de72fca7-9478-4e44-97cd-24f3e9d1ca24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'article_title': 'Multimodal Models\\u200a—\\u200aLLMs that can see and hear',\n",
       "  'section': 'What is a Multimodal Model?',\n",
       "  'text': 'One benefit of using existing LLM as a starting point for MMs is that they’ve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations.'},\n",
       " {'article_title': 'Multimodal Models\\u200a—\\u200aLLMs that can see and hear',\n",
       "  'section': 'Multimodal Models\\u200a—\\u200aLLMs That Can See and\\xa0Hear',\n",
       "  'text': 'This has sparked efforts toward expanding LLM functionality to include multiple modalities.'},\n",
       " {'article_title': 'Multimodal Models\\u200a—\\u200aLLMs that can see and hear',\n",
       "  'section': 'What is a Multimodal Model?',\n",
       "  'text': 'While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3].'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = 5\n",
    "threshold = 0.05\n",
    "\n",
    "# multimodal search over articles\n",
    "text_similarities = matmul(query_embed, text_embeddings.T)\n",
    "image_similarities = matmul(query_embed, image_embeddings.T)\n",
    "\n",
    "# rescale similarities via softmax\n",
    "temp=0.25\n",
    "text_scores = softmax(text_similarities/temp, dim=1)\n",
    "image_scores = softmax(image_similarities/temp, dim=1)\n",
    "\n",
    "# return top k filtered text results\n",
    "isorted_scores = argsort(text_scores, descending=True)[0]\n",
    "sorted_scores = text_scores[0][isorted_scores]\n",
    "\n",
    "itop_k_filtered = [idx.item() for idx, score in zip(isorted_scores, sorted_scores) if score.item() >= threshold][:k]\n",
    "top_k = [text_content_list[i] for i in itop_k_filtered]\n",
    "\n",
    "top_k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca680072-0666-4392-9c6c-02f1b6d2cfab",
   "metadata": {},
   "source": [
    "#### text and image search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e3101da-2b8c-4626-a00a-e7b946fd3d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_results, text_scores = similarity_search(query_embed, text_embeddings, text_content_list, k=15, threshold=0.01, temperature=0.25)\n",
    "image_results, image_scores = similarity_search(query_embed, image_embeddings, image_content_list, k=5, threshold=0.25, temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8b5af69-2577-4ff8-ab94-6924c162538a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* One benefit of using existing LLM as a starting point for MMs is that they’ve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations.\n",
      "* This has sparked efforts toward expanding LLM functionality to include multiple modalities.\n",
      "* While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3].\n",
      "* GPT-4o — Input: text, images, and audio. Output: text.FLUX — Input: text. Output: images.Suno — Input: text. Output: audio.\n",
      "* The downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. Information may degrade going from one module to another via text descriptions only.\n",
      "* Namely, LLMs can only process and generate text, making them blind to other modalities such as images, video, audio, and more. This is a major limitation since some tasks rely on non-text data, e.g., analyzing engineering blueprints, reading body language or speech tonality, and interpreting plots and infographics.\n",
      "* While this approach comes with significantly greater technical challenges and computational requirements, it enables the seamless integration of multiple modalities into a shared concept space, unlocking better reasoning capabilities and efficiencies [10].\n",
      "* Two key aspects of CL contribute to its effectiveness\n",
      "* [2] A Survey on Multimodal Large Language Models\n",
      "* The key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training.\n",
      "* Here, I will focus on multimodal models developed from an LLM. Three popular approaches are described below.\n",
      "* A Multimodal Model (MM) is an AI system that can process multiple data modalities as input or output (or both) [1]. Below are a few examples.\n",
      "* Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs).\n",
      "* [1] Multimodal Machine Learning: A Survey and Taxonomy\n",
      "* LLM + Tools: Augment LLMs with pre-built componentsLLM + Adapters: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuningUnified Models: Expand LLM architecture to fuse modalities at pre-training\n"
     ]
    }
   ],
   "source": [
    "for text in text_results:\n",
    "    if text_results:\n",
    "        print(\"*\", text['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd459d48-4fec-4c58-acbb-2c4944d36c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in image_results:\n",
    "    display(Image(filename=image['image_path']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6df328-7f5c-4aa7-b2b2-2005b7bbf976",
   "metadata": {},
   "source": [
    "### Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342849f-b6d7-4264-93a8-7756fdb623d0",
   "metadata": {},
   "source": [
    "#### format context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bf865ac-a545-4ff4-8e74-931fbe3b638f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_context = \"\"\n",
    "for text in text_results:\n",
    "    if text_results:\n",
    "        text_context = text_context + \"**Article title:** \" + text['article_title'] + \"\\n\"\n",
    "        text_context = text_context + \"**Section:**  \" + text['section'] + \"\\n\"\n",
    "        text_context = text_context + \"**Snippet:** \" + text['text'] + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78f3d078-b479-4b11-a04b-81343fbfe44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_context = \"\"\n",
    "for image in image_results:\n",
    "    if image_results:\n",
    "        image_context = image_context + \"**Article title:** \" + image['article_title'] + \"\\n\"\n",
    "        image_context = image_context + \"**Section:**  \" + image['section'] + \"\\n\"\n",
    "        image_context = image_context + \"**Image Path:**  \" + image['image_path'] + \"\\n\"\n",
    "        image_context = image_context + \"**Image Caption:** \" + image['caption'] + \"\\n\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f27c84-f2f6-492f-9f71-decd02a02cfb",
   "metadata": {},
   "source": [
    "#### prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d496214d-bf46-4859-9e48-aaf94d03328d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct prompt template\n",
    "prompt_template = lambda query, text_context, image_context : f\"\"\"Given the query \"{query}\" and the following relevant snippets:\n",
    "\n",
    "{text_context}\n",
    "{image_context}\n",
    "\n",
    "Please provide a concise and accurate answer to the query, incorporating relevant information from the provided snippets where possible.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dffd4239-b60b-47a9-a19d-d1268a73b4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the query \"What are the three paths described for making LLMs multimodal?\" and the following relevant snippets:\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What is a Multimodal Model?\n",
      "**Snippet:** One benefit of using existing LLM as a starting point for MMs is that they’ve demonstrated a strong ability to acquire world knowledge through large-scale pre-training, which can be leveraged to process concepts appearing in non-textual representations.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  Multimodal Models — LLMs That Can See and Hear\n",
      "**Snippet:** This has sparked efforts toward expanding LLM functionality to include multiple modalities.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What is a Multimodal Model?\n",
      "**Snippet:** While there are several ways to create models that can process multiple data modalities, a recent line of research seeks to use LLMs as the core reasoning engine of a multimodal system [2]. Such models are called multimodal large language models (or large multimodal models) [2][3].\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What is a Multimodal Model?\n",
      "**Snippet:** GPT-4o — Input: text, images, and audio. Output: text.FLUX — Input: text. Output: images.Suno — Input: text. Output: audio.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  Path 1: LLM + Tools\n",
      "**Snippet:** The downside, however, is that the quality of such a system may be limited. Just like when playing a game of telephone, messages mutate when passed from person to person. Information may degrade going from one module to another via text descriptions only.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  Multimodal Models — LLMs That Can See and Hear\n",
      "**Snippet:** Namely, LLMs can only process and generate text, making them blind to other modalities such as images, video, audio, and more. This is a major limitation since some tasks rely on non-text data, e.g., analyzing engineering blueprints, reading body language or speech tonality, and interpreting plots and infographics.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  Path 3: Unified Models\n",
      "**Snippet:** While this approach comes with significantly greater technical challenges and computational requirements, it enables the seamless integration of multiple modalities into a shared concept space, unlocking better reasoning capabilities and efficiencies [10].\n",
      "\n",
      "**Article title:** Multimodal Embeddings: An Introduction\n",
      "**Section:**  Contrastive Learning\n",
      "**Snippet:** Two key aspects of CL contribute to its effectiveness\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What’s next?\n",
      "**Snippet:** [2] A Survey on Multimodal Large Language Models\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  Path 1: LLM + Tools\n",
      "**Snippet:** The key benefit of such an approach is simplicity. Tools can quickly be assembled without any additional model training.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  3 Paths to Multimodality\n",
      "**Snippet:** Here, I will focus on multimodal models developed from an LLM. Three popular approaches are described below.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What is a Multimodal Model?\n",
      "**Snippet:** A Multimodal Model (MM) is an AI system that can process multiple data modalities as input or output (or both) [1]. Below are a few examples.\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What’s next?\n",
      "**Snippet:** Multimodal models are AI systems that can process multiple data modalities as inputs or outputs (or both). A recent trend for developing these systems involves adding modalities to large language models (LLMs).\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  What’s next?\n",
      "**Snippet:** [1] Multimodal Machine Learning: A Survey and Taxonomy\n",
      "\n",
      "**Article title:** Multimodal Models — LLMs that can see and hear\n",
      "**Section:**  3 Paths to Multimodality\n",
      "**Snippet:** LLM + Tools: Augment LLMs with pre-built componentsLLM + Adapters: Augment LLMs with multi-modal encoders or decoders, which are aligned via adapter fine-tuningUnified Models: Expand LLM architecture to fuse modalities at pre-training\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Please provide a concise and accurate answer to the query, incorporating relevant information from the provided snippets where possible.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template(query, text_results, image_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9b10d5-bc0f-49f3-8a6b-f81be4ffa78d",
   "metadata": {},
   "source": [
    "### Prompt LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c3ed795-7430-464e-8d18-503e252ae0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProgressResponse(status='success', completed=None, total=None, digest=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ollama.pull('llama3.2-vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87d986fb-d564-49b1-82c6-c5258cc13ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The three paths described for making Large Language Models (LLMs) multimodal are:\n",
      "\n",
      "1. **LLM + Tools**: This approach involves augmenting an LLM with pre-built components that can process multiple data modalities. The key benefit is simplicity, as tools can be quickly assembled without additional model training. However, the quality of such a system may be limited due to information degradation when passing messages from one module to another via text descriptions only.\n",
      "\n",
      "2. **LLM + Adapters**: This approach involves augmenting an LLM with multi-modal encoders or decoders that are aligned via adapter fine-tuning. Unfortunately, this approach is not explicitly mentioned in the provided snippets, but it can be inferred as a possible method for making LLMs multimodal.\n",
      "\n",
      "3. **Unified Models**: This approach involves expanding the architecture of an LLM to fuse multiple modalities into a shared concept space at pre-training time. While this approach comes with greater technical challenges and computational requirements, it enables seamless integration of multiple modalities and unlocks better reasoning capabilities and efficiencies.\n",
      "\n",
      "These three paths are mentioned in the snippet \"3 Paths to Multimodality\" under the section title \"Multimodal Models — LLMs that can see and hear\".\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template(query, text_results, image_results)\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3.2-vision',\n",
    "    messages=[{\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "        'images': [image[\"image_path\"] for image in image_results]\n",
    "    }]\n",
    ")\n",
    "\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
